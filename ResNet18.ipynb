{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "!pip install kaggle\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "!pip install optuna\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# code for importing and processing the imported data directly from Kaggle\n",
    "drive.mount('/content/drive')\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/kaggle\"\n",
    "!kaggle competitions download -c vub-ml-project-2024-animal-classification\n",
    "!mkdir -p /content/drive/MyDrive/kaggle\n",
    "!unzip vub-ml-project-2024-animal-classification.zip -d /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and preprocessing\n",
    "EPOCH = 15\n",
    "pre_epoch = 0\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "\n",
    "# Define dataset directory\n",
    "train_dir = \"/content/vub-ml-project-2024-animal-classification/train\"\n",
    "\n",
    "# Define transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),      # switch to 224x224 and fix the network so it will work\n",
    "    transforms.RandomCrop(224, padding=16),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Get the number of classes dynamically\n",
    "num_classes = len(train_dataset.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no optimization code\n",
    "# Set hyperparameters\n",
    "EPOCH = 10\n",
    "pre_epoch = 0\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.01\n",
    "\n",
    "# Define dataset directory\n",
    "train_dir = \"/content/vub-ml-project-2024-animal-classification/train\"\n",
    "\n",
    "# Define transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  \n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Get the number of classes dynamically\n",
    "num_classes = len(train_dataset.dataset.classes)\n",
    "\n",
    "# Define ResNet18 with the correct number of output classes\n",
    "net = ResNet(ResidualBlock, num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
    "\n",
    "\n",
    "# StepLR/ExponentialLR: Simple decays for stable training.\n",
    "# CosineAnnealing: For tasks that benefit from smooth decay.\n",
    "# ReduceLROnPlateau: Tasks with validation monitoring needs.\n",
    "# OneCycleLR: Optimized training, often for large-scale tasks.\n",
    "# CyclicLR: Tasks requiring escape from local minima.\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print(f'\\nEpoch: {epoch + 1}')\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics for training\n",
    "        sum_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "\n",
    "        print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%%' %\n",
    "              (epoch + 1, i + 1, sum_loss / (i + 1), 100. * correct / total))\n",
    "\n",
    "    # Validation loop\n",
    "    print('Validating...')\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.data).cpu().sum()\n",
    "\n",
    "    print('Validation Loss: %.3f | Validation Acc: %.3f%%' %\n",
    "          (val_loss / len(val_loader), 100. * correct / total))\n",
    "scheduler.step()\n",
    "print(f'Training complete. Total epochs: {EPOCH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE FUNCTIONALITY OF THE CLASS\n",
    "net = ResNet(ResidualBlock, num_classes=num_classes).to(device)\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "output = net(dummy_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no optimization code\n",
    "# Define ResNet18 with the correct number of output classes\n",
    "net = ResNet(ResidualBlock, num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.4)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader),epochs=EPOCH)\n",
    "\n",
    "\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(pre_epoch, EPOCH):\n",
    "    print(f'\\nEpoch: {epoch + 1}')\n",
    "    net.train()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward and backward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics for training\n",
    "        sum_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).cpu().sum()\n",
    "\n",
    "        print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%%' %\n",
    "              (epoch + 1, i + 1, sum_loss / (i + 1), 100. * correct / total))\n",
    "\n",
    "    # Validation loop\n",
    "    print('Validating...')\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.data).cpu().sum()\n",
    "\n",
    "    print('Validation Loss: %.3f | Validation Acc: %.3f%%' %\n",
    "          (val_loss / len(val_loader), 100. * correct / total))\n",
    "    scheduler.step(val_loss / len(val_loader))\n",
    "\n",
    "#scheduler.step()\n",
    "print(f'Training complete. Total epochs: {EPOCH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define test directory\n",
    "test_dir = \"/content/vub-ml-project-2024-animal-classification/test\"\n",
    "\n",
    "# Define transformations\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load test images and filenames\n",
    "test_images = []\n",
    "image_filenames = []\n",
    "\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(test_dir, filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = transform_test(image)\n",
    "        test_images.append(image)\n",
    "        image_filenames.append(filename)\n",
    "\n",
    "# Stack images into a single tensor\n",
    "test_images = torch.stack(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a DataLoader from the test images\n",
    "test_dataset = TensorDataset(test_images)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define test directory\n",
    "test_dir = \"/content/vub-ml-project-2024-animal-classification/test\"\n",
    "\n",
    "# Define transformations\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load test images and filenames\n",
    "test_images = []\n",
    "image_filenames = []\n",
    "\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "        image_path = os.path.join(test_dir, filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = transform_test(image)\n",
    "        test_images.append(image)\n",
    "        image_filenames.append(filename)\n",
    "\n",
    "# Stack images into a single tensor\n",
    "test_images = torch.stack(test_images)\n",
    "\n",
    "# Create a DataLoader from the test images\n",
    "test_dataset = TensorDataset(test_images)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "import csv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "# List to store predictions\n",
    "test_pred_probs = []\n",
    "\n",
    "# No need to compute gradients during inference\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images[0].to(device)  # Use images[0] to unpack the DataLoader tuple\n",
    "\n",
    "        # Get raw outputs (logits) from the model\n",
    "        outputs = net(images)\n",
    "\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # Store the probabilities\n",
    "        test_pred_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Get the class labels\n",
    "label_strings = train_dataset.dataset.classes\n",
    "\n",
    "# Define the output path for the CSV file\n",
    "output_path = \"./submission/preds.csv\"\n",
    "\n",
    "# Write the predictions to the CSV file\n",
    "def writePredictionsToCsv(predictions, out_path, label_strings):\n",
    "    \"\"\"Writes the predictions to a csv file.\n",
    "    Assumes the predictions are ordered by test interval id.\"\"\"\n",
    "    with open(out_path, 'w') as outfile:\n",
    "        csvwriter = csv.writer(\n",
    "            outfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        # Write the header\n",
    "        row_to_write = ['Id'] + [label for label in label_strings]\n",
    "        csvwriter.writerow(row_to_write)\n",
    "        # Write the rows using 18 digit precision\n",
    "        for idx, prediction in enumerate(predictions):\n",
    "            assert len(prediction) == len(label_strings)\n",
    "            csvwriter.writerow(\n",
    "                [idx + 1] +  # Use an integer for the Id\n",
    "                [\"%.18f\" % p for p in prediction]\n",
    "            )\n",
    "\n",
    "# Call the function to write the predictions to the CSV file\n",
    "writePredictionsToCsv(test_pred_probs, output_path, label_strings)\n",
    "\n",
    "print(f\"Submission file saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opencv-env)",
   "language": "python",
   "name": "opencv-env"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
